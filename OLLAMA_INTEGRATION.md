# Integra√ß√£o com Ollama - Respostas Autom√°ticas

Esta funcionalidade permite que o sistema responda automaticamente √†s mensagens recebidas no WhatsApp usando um modelo de IA rodando localmente via Ollama.

## üìã Pr√©-requisitos

1. ‚úÖ Ollama instalado e rodando no seu computador
2. ‚úÖ Modelo `llama3.1:8b` (ou outro) baixado e dispon√≠vel
3. ‚úÖ Servidor Ollama rodando (padr√£o: `http://localhost:11434`)

## üöÄ Como Instalar o Ollama

### Windows/Mac/Linux

1. Baixe o Ollama em: https://ollama.ai
2. Instale seguindo as instru√ß√µes do site
3. Baixe o modelo que deseja usar:
   ```bash
   ollama pull llama3.1:8b
   ```
4. Verifique se est√° rodando:
   ```bash
   ollama list
   ```

## ‚öôÔ∏è Configura√ß√£o

Adicione as seguintes vari√°veis no arquivo `.env.local`:

```env
# Integra√ß√£o Ollama (Respostas Autom√°ticas)
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_AUTO_REPLY_ENABLED=true
```

### Vari√°veis de Ambiente

- **OLLAMA_URL** (opcional): URL do servidor Ollama. Padr√£o: `http://localhost:11434`
- **OLLAMA_MODEL** (opcional): Nome do modelo a ser usado. Padr√£o: `llama3.1:8b`
- **OLLAMA_AUTO_REPLY_ENABLED** (opcional): Habilita/desabilita respostas autom√°ticas. Padr√£o: `true` (habilitado)

### Desabilitar Respostas Autom√°ticas

Para desabilitar temporariamente, adicione no `.env.local`:

```env
OLLAMA_AUTO_REPLY_ENABLED=false
```

## üîÑ Como Funciona

1. **Mensagem Recebida**: Quando uma mensagem de texto √© recebida via webhook
2. **Hist√≥rico Buscado**: O sistema busca as √∫ltimas 10 mensagens da conversa
3. **Resposta Gerada**: O Ollama gera uma resposta baseada no hist√≥rico e na mensagem atual
4. **Resposta Enviada**: A resposta √© enviada automaticamente via WhatsApp

### Contexto da Conversa

O sistema mant√©m o contexto da conversa incluindo:
- √öltimas 10 mensagens trocadas
- Nome do contato (quando dispon√≠vel)
- Mensagens anteriores para manter coer√™ncia

## üß™ Testando

1. Certifique-se de que o Ollama est√° rodando:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. Envie uma mensagem de teste para o n√∫mero conectado ao WhatsApp Business

3. Verifique os logs no console do servidor para ver:
   - Busca do hist√≥rico
   - Gera√ß√£o da resposta
   - Envio da mensagem

## ‚ö†Ô∏è Limita√ß√µes

- **Apenas mensagens de texto**: Respostas autom√°ticas s√≥ funcionam para mensagens de texto
- **Mensagens com m√≠dia**: Imagens, √°udios e outros tipos n√£o geram respostas autom√°ticas
- **Modelo local**: Requer que o Ollama esteja rodando no mesmo computador ou acess√≠vel via rede

## üîß Troubleshooting

### Erro: "N√£o foi poss√≠vel conectar ao Ollama"

**Solu√ß√£o**: Verifique se o Ollama est√° rodando:
```bash
# Windows (PowerShell)
Test-NetConnection -ComputerName localhost -Port 11434

# Linux/Mac
curl http://localhost:11434/api/tags
```

### Respostas n√£o est√£o sendo enviadas

**Verifique**:
1. `OLLAMA_AUTO_REPLY_ENABLED=true` no `.env.local`
2. O modelo especificado existe (`ollama list`)
3. Os logs do console para erros espec√≠ficos

### Modelo n√£o encontrado

**Solu√ß√£o**: Baixe o modelo novamente:
```bash
ollama pull llama3.1:8b
```

Ou use outro modelo dispon√≠vel e atualize `OLLAMA_MODEL` no `.env.local`

## üìù Exemplo Completo de .env.local

```env
# MongoDB
MONGODB_URL=mongodb+srv://...

# Webhook
WHATSAPP_VERIFY_TOKEN=seu_token_secreto

# WhatsApp API - Envio
WHATSAPP_PHONE_NUMBER_ID=123456789012345
WHATSAPP_ACCESS_TOKEN=seu_access_token

# Ollama - Respostas Autom√°ticas
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
OLLAMA_AUTO_REPLY_ENABLED=true
```

